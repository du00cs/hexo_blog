---
layout: post
title: Spark问题备忘
date: "2014-12-12 17:46"
tags:
- Spark
categories:
- Spark
---

　　想当年被mapreduce虐得死去活来，换上Spark其实是接着虐，记录在此仅作备忘。

### 1. Spark/Scala在Eclipse中的设置
　　IntelliJ IDEA也有相应插件，在此不提。基于Eclipse的方案中可以安装ScalaIDE的插件，但还是建议使用ScalaIDE，而且是[Milestone](http://scala-ide.org/download/milestone.html)版本。IDE的新版本不同于硬件驱动，往往是添加了更好的新功能，不需要坚持用一个老版本，况且开源的东西对老版本的东西感觉好像不维护的样子。  

　　scala工程在Eclipse中有一些常见问题/设置如下：  
- **怎么创建一个scala的maven工程？**  
　　参考[Spark/Scala的入门材料](http://du00cs.github.com/2014/11/11/2014-11-11-spark-scala-introduction/)，建一个新工程，配置好scala的maven编译插件即可。
- **引入一个scala的maven工程后需要哪些设置？**
　　如果IDE没有自动识别出工程为Scala工程，则需要进行以下设置：
- **添加工程的Scala特性**  
　　`工程->右键->Configure->Add Scala Nature`
- **添加Scala代码路径为Source Folder**  
　　比如src/main/scala文件夹下有scala源文件，对着scala文件夹——`右键 ->Build Path -> Use as Source Folder`
- **修改JDK（可能引起问题的）**  
　　修改JDK兼容版本从1.5到1.6——`JRE System Library -> 右键 -> Properties -> 选择Java SE-1.6`
- **修改Scala版本的依赖**  
　　Problems的View中可能会有红色的错误提示说xx.jar是用2.10编译的，而你使用的是2.11，`ctrl+1或者右键-> Quick Fix -> 下拉Scala Installation选择相应版本（2.10)`
- **增加IDE的内存使用**  
　　这个还是很有必要的，`菜单栏->Scala->Run Setup Diagnostics->选中Use recommended default settings`，如果Heap settings中的最大值不到2G就需要修改了，具体来说是修改eclipse.ini。


### 2. org.apache.spark.SparkException: Error communicating with MapOutputTracker
问题：少量数据可以运行成功，而数据量放大到数十倍之后运行失败并报这个错。
```scala
val sparkConf = new SparkConf()
  .setAppName("Host Access Freq Stats")
  .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
  .set("spark.akka.frameSize", "500"); // workers should be able to send bigger  messages
```
